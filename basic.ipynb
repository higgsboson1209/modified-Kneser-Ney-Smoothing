import pandas as pd
import numpy as np
import nltk 
from nltk.tokenize import TreebankWordTokenizer
from nltk import ngrams
from collections import Counter
from tqdm import tqdm
class MKNNGram:
    def __init__(self,n,corpus):
        self.highest_n =int(n)
        self.discounts=np.zeros((self.highest_n,3))
        self.corpus=corpus
        self.ngram_probabilities={}
        self.ngram_freqs=[]
        self.cumulative_frequency_summations=[]
    
    def _get_padded_ngrams(self, corpus, n):
        """
        start marker = <s>
        stop marker =  <\s>
        :param corpus:  Pandas Dataframe.  Corpus to be padded
        :return: list of padded ngrams
        """
        n_grams = []
        for index,row in corpus.iterrows():
            tmp_ngrams = ngrams(row["text"], n, pad_left=True, pad_right=True,
                                left_pad_symbol='<s>', right_pad_symbol="</s>")
            n_grams.extend(tmp_ngrams)

        return n_grams

    def _get_all_padded_ngrams(self):
        """
        Finds padded ngrams for each for degrees 1 to highest order.
        :return: padded ngrams
        """
        return [self._get_padded_ngrams(self.corpus, i)
                for i in range(1, self.highest_n+ 1)]

    def _calc_ngram_freqs(self, padded_ngrams):
        """
        Finds the frequencies of each ngram for degrees 1 to highest order.
        :return: ngram frequencies
        """
        for i in range(0, self.highest_n):
            if i != 0:
                freqs = Counter(padded_ngrams[i])
                # sorting our dictionary
                sorted(freqs.items(), key=lambda x: x[1])
                self.ngram_freqs.append(freqs)

            else:
                freqs = Counter(padded_ngrams[i])
                # sorting our dictionary
                sorted(freqs.items(), key=lambda x: x[1])
                self.ngram_freqs.append(freqs)

        # need to account for the end pad (probability that a sentence will end)
        if self.highest_n> 1:
            # fun with generators
            end_of_sentence_count = sum(self.ngram_freqs[1].get(key) for key in self.ngram_freqs[1]
                                        if key[-1] == "</s>")
            self.ngram_freqs[0].update(({("</s>",): end_of_sentence_count}))
            self.ngram_freqs[0].update(({("<s>",): end_of_sentence_count}))
    
    def _calc_discounts(self):
        """
        Calculates the discount values for each ngram degree.
        :return: None
        """
        for i in range(0, self.highest_n):
            freq_counter=self.ngram_freqs[i]
            n1,n2,n3,n4=0,0,0,0
            for key,value in freq_counter.items():
                if value==1:
                    n1+=1
                elif value==2:
                    n2+=1
                elif value==3:
                    n3+=1
                elif value==4:
                    n4+=1
            Y=n1/(n1+2*n2)
            #D1,D2,D3+
            self.discounts[i][0]=1-2*Y*(n2/n1)
            self.discounts[i][1]=2-3*Y*(n3/n2)
            self.discounts[i][2]=3-4*Y*(n4/n3)
    
    def _calc_gamma(self,sequence,cum_sum_for_order):
        order_n=len(sequence)
        gamma=self.discounts[order_n,0]*self._calc_N(1,pre=sequence,post=None,sign=False) + self.discounts[order_n,1]*self._calc_N(2,pre=sequence,post=None,sign=False) + self.discounts[order_n,2]*self._calc_N(3,pre=sequence,post=None,sign=True) 
        gamma=gamma/cum_sum_for_order
        return gamma

    def _calc_ngram_probability(self,sequence):
        if sequence in self.ngram_probabilities:
            return self.ngram_probabilities[sequence]
        n_gram_order=len(sequence)
        print(n_gram_order)
        freq_counter=self.ngram_freqs[n_gram_order-1]
        if n_gram_order==1:
            #Base Case for Unigrams
            self.ngram_probabilities[sequence]=self._calc_N(1,pre=None,post=sequence,sign=True)/self._calc_N(1,pre=None,post=None,sign=True)
        else:
            main_count_value=freq_counter[sequence]
            print("The number of times the sequence appeared in the corpus is",main_count_value)
            c_sum_n_gram_order=self.cumulative_frequency_summations[n_gram_order-1] 
            print("The cumulative sum denominator for the order ",n_gram_order, " is ",c_sum_n_gram_order)
            applied_discount=self._use_Discount(main_count_value,n_gram_order-1)
            print("The discount applied to the sequence is ",applied_discount)
            primary_probability=(main_count_value-applied_discount)/c_sum_n_gram_order
            print("The primary probability is",primary_probability)
            print("Gamma is being calculated for the sequence",sequence[:-1])
            gamma_value=self._calc_gamma(sequence[:-1],c_sum_n_gram_order)
            print("The gamma value is",gamma_value)
            print("The lower order probability is being calculated for the sequence",sequence[1:])
            lower_order_gram_probability=self._calc_ngram_probability(sequence[1:])
            print("The lower order gram probability is",lower_order_gram_probability)
            smoothing_via_interpolation=gamma_value*lower_order_gram_probability
            print("The smoothing via interpolation is",smoothing_via_interpolation)
            self.ngram_probabilities[sequence]=primary_probability+smoothing_via_interpolation
            print("The final probability is",self.ngram_probabilities[sequence])
        return self.ngram_probabilities[sequence]

    def _use_Discount(self,count_value,n_gram_order):
        if count_value==0:
            return 0
        elif count_value==1:
            return self.discounts[n_gram_order,0]
        elif count_value==2:
            return self.discounts[n_gram_order,1]
        elif count_value>=3:
            return self.discounts[n_gram_order,2]
    def _calc_N(self,counter_value,pre=None,post=None,sign=None):
        if not pre and not post:
            """Base case Denominator, return count of all bigrams where frequency >0"""
            N=0
            for key,value in self.ngram_freqs[1].items():
                if value>0:
                    N+=1
            return N
        if not pre:
            N=0
            post=list(post)
            for key,value in self.ngram_freqs[len(post)].items():
                if sign: 
                    if list(key[1:])==post and value>=counter_value:
                        N+=1 
                else: 
                    if list(key[1:])==post and value==counter_value:
                        N+=1
            return N
        if not post: 
            N=0
            pre=list(pre)
            for key,value in self.ngram_freqs[len(pre)].items():
                if sign: 
                    if list(key[:-1])==pre and value>=counter_value:
                        N+=1 
                else: 
                    if list(key[:-1])==pre and value==counter_value:
                        N+=1
            return N

    def _calc_cumfreqs(self):
        """
        Calculates the cumulative frequency summations for each ngram degree.
        :return: None
        """
        for i in range(0, self.highest_n):
            freq_counter=self.ngram_freqs[i]
            cumulative_sum=0
            for key,value in freq_counter.items():
                cumulative_sum+=value
            self.cumulative_frequency_summations.append(cumulative_sum)
    def _train(self):
        """
        Trains the model by calculating the discount values and ngram probabilities.
        :return: None
        """
        # get all padded ngrams
        padded_ngrams = self._get_all_padded_ngrams()
        # get all ngram frequencies
        self._calc_ngram_freqs(padded_ngrams)
        self._calc_cumfreqs()
        self._calc_discounts()
    def _perplexity(self):
        pass
    
        



data_frame=pd.read_csv("/Users/higgs/Downloads/Data for Toy tasks/finalenglishdata.txt",sep="\t",header=None,names=["text"])
tokenizer=TreebankWordTokenizer()
data_frame['text'] = data_frame.apply(lambda row: tokenizer.tokenize(row['text']), axis=1)
data_frame=data_frame[:10000]
mk_object=MKNNGram(4,data_frame)
mk_object._train()
